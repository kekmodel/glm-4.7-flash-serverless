{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLM-4.7-Flash RunPod Endpoint Test\n",
    "\n",
    "테스트 항목:\n",
    "1. Health Check\n",
    "2. Model Info\n",
    "3. Chat Completions\n",
    "4. Generate Endpoint\n",
    "5. Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"RUNPOD_API_KEY\")\n",
    "ENDPOINT_ID = os.getenv(\"ENDPOINT_ID\")\n",
    "BASE_URL = f\"https://api.runpod.ai/v2/{ENDPOINT_ID}\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "print(f\"Endpoint: {ENDPOINT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(f\"{BASE_URL}/health\", headers=headers)\n",
    "print(f\"Status: {response.status_code}\")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"input\": {\n",
    "        \"openai_route\": \"/v1/models\",\n",
    "        \"openai_input\": {}\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(f\"{BASE_URL}/runsync\", headers=headers, json=payload)\n",
    "print(f\"Status: {response.status_code}\")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chat Completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"input\": {\n",
    "        \"openai_route\": \"/v1/chat/completions\",\n",
    "        \"openai_input\": {\n",
    "            \"model\": \"glm-4.7-flash\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": \"안녕! 자기소개 해줘.\"}\n",
    "            ],\n",
    "            \"max_tokens\": 256,\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(f\"{BASE_URL}/runsync\", headers=headers, json=payload, timeout=300)\n",
    "print(f\"Status: {response.status_code}\")\n",
    "result = response.json()\n",
    "print(result)\n",
    "\n",
    "if \"output\" in result:\n",
    "    output = result[\"output\"]\n",
    "    if \"choices\" in output:\n",
    "        print(\"\\n--- Response ---\")\n",
    "        print(output[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Endpoint (SGLang Native)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"input\": {\n",
    "        \"openai_route\": \"/generate\",\n",
    "        \"openai_input\": {\n",
    "            \"text\": \"def fibonacci(n):\",\n",
    "            \"sampling_params\": {\n",
    "                \"max_new_tokens\": 128,\n",
    "                \"temperature\": 0.5\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(f\"{BASE_URL}/runsync\", headers=headers, json=payload, timeout=300)\n",
    "print(f\"Status: {response.status_code}\")\n",
    "result = response.json()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tokenizer Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"zai-org/GLM-4.7-Flash\", trust_remote_code=True)\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Model max length: {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize test\n",
    "text = \"안녕하세요! GLM-4.7-Flash 테스트입니다.\"\n",
    "tokens = tokenizer.encode(text)\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Token count: {len(tokens)}\")\n",
    "print(f\"Decoded: {tokenizer.decode(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat template test\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "]\n",
    "\n",
    "formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(\"Chat template:\")\n",
    "print(formatted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
